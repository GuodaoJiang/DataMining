SVM擅长解决数据线性不可分的问题。（线性可不可分取决于两个数据集的凸包有没有重叠，换言之，能不能用多个线将两个数据集完全分开）
1.用核函数接受两个低维空间的向量，能够计算出经过某个变换后在高维空间里的向量内积，即将线性不可分问题转化成线性可分问题。
2.惩罚因子的作用，找到最好的超平面，即尽可能地将数据正确分类。

类型：OVR SVMS、OVO SVMS、 DAG SVM
类型之间的区别见博客:https://blog.csdn.net/xfchen2/article/details/79621396

OVR SVMS:
优点:训练k个分类器，个数较少，其分类速度相对较快。

缺点:1.每个分类器的训练都是将全部的样本作为训练样本，这样在求解二次规划问题时，训练速度会随着训练样本的数量的增加而急剧减慢；
     2.同时由于负类样本的数据要远远大于正类样本的数据，从而出现了样本不对称的情况，且这种情况随着训练数据的增加而趋向严重。解决不对称的问题可以引入不同的惩罚因子，对样本点来说较少的正类采用较大的惩罚因子C；
     3.还有就是当有新的类别加进来时，需要对所有的模型进行重新训练。
     
OVO SVMS:
优点：不需要重新训练所有的SVM，只需要重新训练和增加语音样本相关的分类器。在训练单个模型时，相对速度较快。

缺点：所需构造和测试的二值分类器的数量关于k成二次函数增长，总训练时间和测试时间相对较慢。
